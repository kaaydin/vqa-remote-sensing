{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "        question = sample[\"question\"]\n",
    "        answer = sample[\"answer\"]\n",
    "        question_type = sample[\"type\"]\n",
    "        img_id = sample[\"img_id\"]        \n",
    "        \n",
    "        return question, answer, question_type, img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import DS libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Import Torch libraries\n",
    "import torch\n",
    "\n",
    "## Import other libraries\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "## Import HuggingFace libraries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "## Import custom functions\n",
    "from utils.text_preprocessing_utils import *\n",
    "\n",
    "## Import VocabEncoder\n",
    "from utils.VocabEncoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set save path for embeddings\n",
    "SAVE_PATH = \"../data/text_representations_bert_att/\"\n",
    "SAVE_PATH_TRAIN = SAVE_PATH + \"train/\"\n",
    "SAVE_PATH_TEST = SAVE_PATH + \"test/\"\n",
    "SAVE_PATH_VAL = SAVE_PATH + \"val/\"\n",
    "\n",
    "## Check if path exists, if not create it\n",
    "create_path(SAVE_PATH)\n",
    "create_path(SAVE_PATH_TRAIN)\n",
    "create_path(SAVE_PATH_TEST)\n",
    "create_path(SAVE_PATH_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set batch size\n",
    "SAVE_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main data path\n",
    "DATA_PATH = \"../../data/text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set question paths (train, test, val)\n",
    "PATH_questions_split_train = DATA_PATH + \"USGS_split_train_questions.json\"\n",
    "PATH_questions_split_test = DATA_PATH + \"USGS_split_test_questions.json\"\n",
    "PATH_questions_split_val = DATA_PATH + \"USGS_split_val_questions.json\"\n",
    "\n",
    "PATH_questions = [PATH_questions_split_train, PATH_questions_split_test, PATH_questions_split_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set answer paths (train, test, val)\n",
    "PATH_answers_split_train = DATA_PATH + \"USGS_split_train_answers.json\"\n",
    "PATH_answers_split_test = DATA_PATH + \"USGS_split_test_answers.json\"\n",
    "PATH_answers_split_val = DATA_PATH + \"USGS_split_val_answers.json\"\n",
    "\n",
    "PATH_answers = [PATH_answers_split_train, PATH_answers_split_test, PATH_answers_split_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set all paths (questions, answers)\n",
    "PATH_all_questions = DATA_PATH + \"USGSquestions.json\"\n",
    "PATH_all_answers = DATA_PATH + \"USGSanswers.json\"\n",
    "\n",
    "PATH_all = [PATH_all_questions, PATH_all_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve questions and answers\n",
    "questions = [json_to_dataframe(path, \"questions\") for path in PATH_questions]\n",
    "answers = [json_to_dataframe(path, \"answers\") for path in PATH_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove NaNs\n",
    "questions_nan = [remove_nan_rows(question, \"question\") for question in questions]\n",
    "answers_nan = [remove_nan_rows(answer, \"answer\") for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove unnecessary columns\n",
    "questions_clean = [remove_columns(question, [\"active\", \"date_added\", \"people_id\", \"answers_ids\"]) for question in questions_nan]\n",
    "answers_clean = [remove_columns(answer, [\"active\", \"date_added\", \"people_id\", \"question_id\"]) for answer in answers_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate question & answers to generate train, test and val sets\n",
    "train = merge_dataframes_on_column(questions_clean[0], answers_clean[0], \"id\")\n",
    "test = merge_dataframes_on_column(questions_clean[1], answers_clean[1], \"id\")\n",
    "val = merge_dataframes_on_column(questions_clean[2], answers_clean[2], \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate & save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the VocabEncoder objects\n",
    "encoder_answers = VocabEncoder(PATH_all_answers, questions = False, range_numbers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â Instantiate the tokenizer & BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.pooler = None  # Discard the pooling layer\n",
    "model.eval()\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625340it [00:47, 13163.96it/s]\n",
      "105647it [00:07, 13945.72it/s]\n",
      "102843it [00:07, 14021.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Token Length - Train:  35\n",
      "Max Token Length - Test:  28\n",
      "Max Token Length - Val:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Check all lengths in tokenized form\n",
    "lengths_train = length_checker(train, tokenizer=tokenizer)\n",
    "lengths_test = length_checker(test, tokenizer=tokenizer)\n",
    "lengths_val = length_checker(val, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Max Token Length - Train: \", max(lengths_train))\n",
    "print(\"Max Token Length - Test: \", max(lengths_test))\n",
    "print(\"Max Token Length - Val: \", max(lengths_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question_type_distribution(dataframe):\n",
    "    \"\"\"\n",
    "    This function analyzes the distribution of question types in a given dataframe.\n",
    "    \"\"\"\n",
    "    ## Get the number of questions per type\n",
    "    question_type_counts = dataframe[\"type\"].value_counts()\n",
    "    ## Get the total number of questions\n",
    "    total_questions = dataframe.shape[0]\n",
    "    ## Get the percentage of questions per type\n",
    "    question_type_percentages = [round((count / total_questions) * 100, 2) for count in question_type_counts]\n",
    "    ## Create a dataframe with the results\n",
    "    df = pd.DataFrame({\"count\": question_type_counts, \"percentage\": question_type_percentages})\n",
    "    ## Sort by percentage\n",
    "    df = df.sort_values(by=\"percentage\", ascending=False)\n",
    "    ## Return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comp</th>\n",
       "      <td>72923</td>\n",
       "      <td>32.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presence</th>\n",
       "      <td>58545</td>\n",
       "      <td>26.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58149</td>\n",
       "      <td>26.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>33067</td>\n",
       "      <td>14.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count  percentage\n",
       "comp      72923       32.75\n",
       "presence  58545       26.29\n",
       "count     58149       26.11\n",
       "area      33067       14.85"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_question_type_distribution(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representations(dataframe, model, tokenizer, save_path, device):\n",
    "    \n",
    "    save_idx = 0\n",
    "\n",
    "    ## Iterate over items\n",
    "    for idx in tqdm(range(len(dataframe))):\n",
    "            \n",
    "            \n",
    "            ## Retrieve information\n",
    "            question = dataframe.iloc[idx][\"question\"]\n",
    "            answer = dataframe.iloc[idx][\"answer\"]\n",
    "            question_type = dataframe.iloc[idx][\"type\"]\n",
    "            img_id = dataframe.iloc[idx][\"img_id\"]\n",
    "\n",
    "            answer = encoder_answers.encode(answer)\n",
    "\n",
    "            answer = torch.tensor(answer, dtype=torch.long)\n",
    "    \n",
    "            ## Remove question sign from question \n",
    "            # batch_questions = [question[:-1] if question[-1:] == \"?\" else question for question in batch_questions]\n",
    "            if question[-1] == \"?\":\n",
    "                question = question[:-1]\n",
    "            else:\n",
    "                question = question\n",
    "    \n",
    "            ## Retrieve question embedding\n",
    "            # batch_tokenized = tokenizer.batch_encode_plus(batch_questions, pad_to_multiple_of=40, add_special_tokens=True, return_attention_mask=True, padding=True, return_tensors=\"pt\")\n",
    "            input = tokenizer.encode_plus(question, pad_to_multiple_of=35, add_special_tokens=True, return_attention_mask=True, padding=True, return_tensors=\"pt\")\n",
    "            input.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(**input)\n",
    "                hidden_states = output.last_hidden_state.squeeze(0).detach().cpu()\n",
    "    \n",
    "            ## Create data dictionary\n",
    "            data = {\n",
    "                'question': hidden_states,\n",
    "                'answer': answer,\n",
    "                'question_type': question_type,\n",
    "                'image_id': img_id\n",
    "                }\n",
    "    \n",
    "                ## Append data to batch\n",
    "                ## batch_data.append(data)\n",
    "            \n",
    "            ## Save item\n",
    "            save_path_idx = os.path.join(save_path, f\"{save_idx}.pt\")\n",
    "            torch.save(data, save_path_idx)\n",
    "            save_idx += 1\n",
    "\n",
    "            ## Update save_idx & clear all lists\n",
    "            # questions.clear()\n",
    "            # answers.clear()\n",
    "            # question_types.clear()\n",
    "            # img_ids.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625340 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 625340/625340 [1:56:30<00:00, 89.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "## For Train\n",
    "create_representations(train, model, tokenizer, SAVE_PATH_TRAIN, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 105647/105647 [20:17<00:00, 86.79it/s] \n"
     ]
    }
   ],
   "source": [
    "## For Test\n",
    "create_representations(test, model, tokenizer, SAVE_PATH_TEST, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 102843/102843 [20:32<00:00, 83.42it/s]\n"
     ]
    }
   ],
   "source": [
    "## For Val\n",
    "create_representations(val, model, tokenizer, SAVE_PATH_VAL, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"../preprocessed_data/text_representations_bert/train/batch_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][\"question\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
