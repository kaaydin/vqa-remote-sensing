{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parent directory\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import DS libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Import Torch libraries\n",
    "import torch\n",
    "\n",
    "## Import other libraries\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "## Import HuggingFace libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "## Import custom libraries\n",
    "#from utils.processing.textual import *\n",
    "from VQA_model import VocabEncoder as VE\n",
    "import VQA_model.models.seq2vec as seq2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set save path for embeddings\n",
    "SAVE_PATH = \"../../data/text_representations_bert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set batch size\n",
    "BATCH_SIZE = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set question paths (train, test, val)\n",
    "PATH_questions_split_train = '../../data/text/USGS_split_train_questions.json'\n",
    "PATH_questions_split_test = '../../data/text/USGS_split_test_questions.json'\n",
    "PATH_questions_split_val = '../../data/text/USGS_split_val_questions.json'\n",
    "\n",
    "PATH_questions = [PATH_questions_split_train, PATH_questions_split_test, PATH_questions_split_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set answer paths (train, test, val)\n",
    "PATH_answers_split_train = '../../data/text/USGS_split_train_answers.json'\n",
    "PATH_answers_split_test = '../../data/text/USGS_split_test_answers.json'\n",
    "PATH_answers_split_val = '../../data/text/USGS_split_val_answers.json'\n",
    "\n",
    "PATH_answers = [PATH_answers_split_train, PATH_answers_split_test, PATH_answers_split_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set all paths (questions, answers)\n",
    "PATH_all_questions = '../../data/text/USGSquestions.json'\n",
    "PATH_all_answers = '../../data/text/USGSanswers.json'\n",
    "\n",
    "PATH_all = [PATH_all_questions, PATH_all_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def json_to_dataframe(json_file_path, delimiter):\n",
    "    \"\"\"\n",
    "    This function converts a JSON file to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    json_file_path : str : the path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    df : DataFrame : a pandas DataFrame created from the JSON file, or\n",
    "    None : if an error occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Open the JSON file\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            # Load the content of the file\n",
    "            # Assuming the JSON structure is a flat dictionary-like structure\n",
    "            # If the structure is different, this line may need adjustment\n",
    "            json_data = json.load(json_file)[delimiter]\n",
    "        \n",
    "        # Convert the JSON data to a DataFrame\n",
    "        # Note: Depending on the JSON structure, you might need a different approach\n",
    "        df = pd.DataFrame(json_data)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {json_file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error occurred while decoding JSON from file: {json_file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch any other exceptions that occur\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve questions and answers\n",
    "questions = [json_to_dataframe(path, \"questions\") for path in PATH_questions]\n",
    "answers = [json_to_dataframe(path, \"answers\") for path in PATH_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows(df, delimiter):\n",
    "    \"\"\"\n",
    "    Remove rows with NaN in the 'question' column from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The modified DataFrame with rows containing NaN in 'question' column removed.\n",
    "    \"\"\"\n",
    "    # Validate if 'question' column exists in the DataFrame\n",
    "    if delimiter in df.columns:\n",
    "        # Remove rows where 'question' column is NaN\n",
    "        df_clean = df.dropna(subset=[delimiter])\n",
    "        return df_clean\n",
    "    else:\n",
    "        raise ValueError(f\"No {delimiter} column found in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(dataframe, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Remove specified columns from a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The original DataFrame.\n",
    "    columns_to_remove (list): A list of column names to remove.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    # Check if all columns to remove are in the DataFrame\n",
    "    for col in columns_to_remove:\n",
    "        if col not in dataframe.columns:\n",
    "            raise ValueError(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Drop the columns\n",
    "    dataframe = dataframe.drop(columns=columns_to_remove)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes_on_column(df1, df2, common_column, how='inner'):\n",
    "    \"\"\"\n",
    "    Merge two pandas DataFrames on a specific common column.\n",
    "\n",
    "    Parameters:\n",
    "    df1 (pd.DataFrame): The first DataFrame.\n",
    "    df2 (pd.DataFrame): The second DataFrame.\n",
    "    common_column (str): The name of the common column to merge on.\n",
    "    how (str): Type of merge to be performed ('left', 'right', 'outer', 'inner'), default is 'inner'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame resulting from the merge of the two input DataFrames.\n",
    "    \"\"\"\n",
    "    # Check if the common column exists in both DataFrames\n",
    "    if common_column not in df1.columns or common_column not in df2.columns:\n",
    "        raise ValueError(f\"The common column '{common_column}' must exist in both DataFrames.\")\n",
    "\n",
    "    # Merge the DataFrames on the common_column\n",
    "    result = pd.merge(df1, df2, on=common_column, how=how)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove NaNs\n",
    "questions_nan = [remove_nan_rows(question, \"question\") for question in questions]\n",
    "answers_nan = [remove_nan_rows(answer, \"answer\") for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove unnecessary columns\n",
    "questions_clean = [remove_columns(question, [\"active\", \"date_added\", \"people_id\", \"answers_ids\"]) for question in questions_nan]\n",
    "answers_clean = [remove_columns(answer, [\"active\", \"date_added\", \"people_id\", \"question_id\"]) for answer in answers_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate question & answers to generate train, test and val sets\n",
    "train = merge_dataframes_on_column(questions_clean[0], answers_clean[0], \"id\")\n",
    "test = merge_dataframes_on_column(questions_clean[1], answers_clean[1], \"id\")\n",
    "val = merge_dataframes_on_column(questions_clean[2], answers_clean[2], \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the VocabEncoder objects\n",
    "encoder_questions = VE.VocabEncoder(PATH_all_questions, questions = True)\n",
    "encoder_answers = VE.VocabEncoder(PATH_all_answers, questions = False, range_numbers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create vocabulary\n",
    "vocabulary_questions = encoder_questions.getVocab()\n",
    "vocabulary_answers = encoder_answers.getVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 6/929256 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianUniSkip(\n",
       "  (embedding): Embedding(145, 620, padding_idx=0)\n",
       "  (rnn): BayesianGRU(\n",
       "    (gru_cell): BayesianGRUCell(\n",
       "      (weight_ir): Linear(in_features=620, out_features=2400, bias=True)\n",
       "      (weight_ii): Linear(in_features=620, out_features=2400, bias=True)\n",
       "      (weight_in): Linear(in_features=620, out_features=2400, bias=True)\n",
       "      (weight_hr): Linear(in_features=2400, out_features=2400, bias=False)\n",
       "      (weight_hi): Linear(in_features=2400, out_features=2400, bias=False)\n",
       "      (weight_hn): Linear(in_features=2400, out_features=2400, bias=False)\n",
       "      (drop_ir): SequentialDropout(0.2500)\n",
       "      (drop_ii): SequentialDropout(0.2500)\n",
       "      (drop_in): SequentialDropout(0.2500)\n",
       "      (drop_hr): SequentialDropout(0.2500)\n",
       "      (drop_hi): SequentialDropout(0.2500)\n",
       "      (drop_hn): SequentialDropout(0.2500)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create the seq2vec object\n",
    "seq2vec = seq2vec.factory(vocabulary_questions, {'arch': 'skipthoughts', 'dir_st': 'data/skip-thoughts', 'type': 'BayesianUniSkip', 'dropout': 0.25, 'fixed_emb': False})\n",
    "for param in seq2vec.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "seq2vec.eval()\n",
    "seq2vec.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c4b9b4f90e4b5e9396850052d5ccf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damia\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\damia\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba1f18543a840cab51de85e11138d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21129ed62bfa4acb897e5f0496687ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797b5617d2e94978957041ab93af4135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b16bdc4e7646398779878311d2d60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create the BERT object --> to be included later in the model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = train.iloc[0].question\n",
    "tokenizer_testing = tokenizer.encode_plus(testing, add_special_tokens=True, return_attention_mask=True, return_tensors=\"pt\")\n",
    "output = bert(**tokenizer_testing)\n",
    "last_hidden_states = output.last_hidden_state\n",
    "cls_embeddings = last_hidden_states[:, 0, :]\n",
    "cls_embeddings = cls_embeddings.squeeze(0)\n",
    "cls_embeddings = cls_embeddings.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(dataframe, batch_size, model, tokenizer, save_path):\n",
    "    \n",
    "    ## Batch Data Collector\n",
    "    batch_data = []\n",
    "    start_idx = 0\n",
    "    save_idx = 0\n",
    "\n",
    "    while start_idx < len(dataframe):\n",
    "    \n",
    "        for idx in tqdm(range(start_idx, min(start_idx + batch_size, len(dataframe)))):\n",
    "            \n",
    "            ## Retrieve row information\n",
    "            row = dataframe.iloc[idx]\n",
    "            question = row.question\n",
    "            answer = row.answer\n",
    "            question_type = row.type\n",
    "            img_id = row.img_id\n",
    "\n",
    "            ## Remove question sign from question (is done automatically for RNN)\n",
    "            if question[:-1] == \"?\":\n",
    "                question = question[:-1]\n",
    "            else:\n",
    "                question = question\n",
    "\n",
    "            ## Retireve answer encoding\n",
    "            answer_encoded = encoder_answers.encode(answer)\n",
    "            answer_tensor = torch.tensor(answer_encoded, dtype=torch.long)\n",
    "            answer_tensor = answer_tensor.cpu().detach()\n",
    "\n",
    "            ## Retrieve question embedding\n",
    "            tokenized = tokenizer.encode_plus(question, add_special_tokens=True, return_attention_mask=True, return_tensors=\"pt\")\n",
    "            output = model(**tokenized)\n",
    "            last_hidden_states = output.last_hidden_state\n",
    "            cls_embeddings = last_hidden_states[:, 0, :].squeeze(0).detach().cpu()\n",
    "\n",
    "            ## Create data dictionary\n",
    "            data = {\n",
    "                'question': cls_embeddings,\n",
    "                'answer': answer_tensor,\n",
    "                'question_type': question_type,\n",
    "                'image_id': img_id\n",
    "                }\n",
    "\n",
    "            ## Append data to batch\n",
    "            batch_data.append(data)\n",
    "\n",
    "        ## Save batch\n",
    "        batch_save_path = os.path.join(save_path, f\"batch_{save_idx}.pt\")\n",
    "        torch.save(batch_data, batch_save_path)\n",
    "        \n",
    "        ## Update indices & clear batch\n",
    "        start_idx += len(batch_data)\n",
    "        save_idx += 1\n",
    "        batch_data.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [02:47<00:00, 23.87it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.50it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.48it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.54it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.65it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.82it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.89it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.82it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.83it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.89it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.53it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.71it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.68it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.56it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.73it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.63it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.62it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.67it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.76it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.65it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.70it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.82it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.64it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.71it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.50it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.89it/s]\n",
      "100%|██████████| 4000/4000 [02:29<00:00, 26.79it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.92it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.83it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.68it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.79it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.95it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.76it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:53<00:00, 23.02it/s]\n",
      "100%|██████████| 4000/4000 [02:38<00:00, 25.29it/s]\n",
      "100%|██████████| 4000/4000 [02:38<00:00, 25.29it/s]\n",
      "100%|██████████| 4000/4000 [02:38<00:00, 25.25it/s]\n",
      "100%|██████████| 4000/4000 [02:38<00:00, 25.17it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.89it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.63it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.89it/s]\n",
      "100%|██████████| 4000/4000 [02:42<00:00, 24.56it/s]\n",
      "100%|██████████| 4000/4000 [02:39<00:00, 25.04it/s]\n",
      "100%|██████████| 4000/4000 [02:39<00:00, 25.05it/s]\n",
      "100%|██████████| 4000/4000 [02:39<00:00, 25.09it/s]\n",
      "100%|██████████| 4000/4000 [02:37<00:00, 25.47it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.83it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.89it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.95it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.73it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.76it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:36<00:00, 25.58it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.77it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.79it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.77it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.79it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.70it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.73it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.70it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.71it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.77it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.77it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.71it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.93it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.93it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.92it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.93it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.90it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.94it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.82it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.83it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 1340/1340 [00:51<00:00, 25.78it/s]\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"data/text_representations_bert/train\"\n",
    "create_batch(train, BATCH_SIZE, bert, tokenizer, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [02:33<00:00, 26.01it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.96it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 25.99it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 26.03it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 25.98it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.92it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 26.01it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.93it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.93it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 26.05it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 25.98it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.96it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.97it/s]\n",
      "100%|██████████| 4000/4000 [02:43<00:00, 24.43it/s]\n",
      "100%|██████████| 4000/4000 [02:33<00:00, 25.98it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.95it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.95it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.96it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.83it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.79it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.77it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.77it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.83it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.88it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.86it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.85it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.87it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.84it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.66it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.91it/s]\n",
      "100%|██████████| 4000/4000 [02:28<00:00, 26.95it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.79it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.76it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.78it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.74it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.76it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.75it/s]\n",
      "100%|██████████| 4000/4000 [02:34<00:00, 25.81it/s]\n",
      "100%|██████████| 4000/4000 [02:35<00:00, 25.80it/s]\n",
      "100%|██████████| 2684/2684 [01:43<00:00, 25.91it/s]\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"data/text_representations_bert/test\"\n",
    "create_batch(test, BATCH_SIZE, bert, tokenizer, save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
