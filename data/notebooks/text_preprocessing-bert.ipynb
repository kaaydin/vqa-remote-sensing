{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "        question = sample[\"question\"]\n",
    "        answer = sample[\"answer\"]\n",
    "        question_type = sample[\"type\"]\n",
    "        img_id = sample[\"img_id\"]        \n",
    "        \n",
    "        return question, answer, question_type, img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import DS libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Import Torch libraries\n",
    "import torch\n",
    "\n",
    "## Import other libraries\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "## Import HuggingFace libraries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "## Import custom functions\n",
    "from utils.text_preprocessing_utils import *\n",
    "\n",
    "## Import VocabEncoder\n",
    "from utils.VocabEncoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set save path for embeddings\n",
    "SAVE_PATH = \"../preprocessed_data/text_representations_bert/\"\n",
    "SAVE_PATH_TRAIN = SAVE_PATH + \"train/\"\n",
    "SAVE_PATH_TEST = SAVE_PATH + \"test/\"\n",
    "SAVE_PATH_VAL = SAVE_PATH + \"val/\"\n",
    "\n",
    "## Check if path exists, if not create it\n",
    "create_path(SAVE_PATH)\n",
    "create_path(SAVE_PATH_TRAIN)\n",
    "create_path(SAVE_PATH_TEST)\n",
    "create_path(SAVE_PATH_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set batch size\n",
    "SAVE_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main data path\n",
    "DATA_PATH = \"../raw_data/text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set question paths (train, test, val)\n",
    "PATH_questions_split_train = DATA_PATH + \"USGS_split_train_questions.json\"\n",
    "PATH_questions_split_test = DATA_PATH + \"USGS_split_test_questions.json\"\n",
    "PATH_questions_split_val = DATA_PATH + \"USGS_split_val_questions.json\"\n",
    "\n",
    "PATH_questions = [PATH_questions_split_train, PATH_questions_split_test, PATH_questions_split_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set answer paths (train, test, val)\n",
    "PATH_answers_split_train = DATA_PATH + \"USGS_split_train_answers.json\"\n",
    "PATH_answers_split_test = DATA_PATH + \"USGS_split_test_answers.json\"\n",
    "PATH_answers_split_val = DATA_PATH + \"USGS_split_val_answers.json\"\n",
    "\n",
    "PATH_answers = [PATH_answers_split_train, PATH_answers_split_test, PATH_answers_split_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set all paths (questions, answers)\n",
    "PATH_all_questions = DATA_PATH + \"USGSquestions.json\"\n",
    "PATH_all_answers = DATA_PATH + \"USGSanswers.json\"\n",
    "\n",
    "PATH_all = [PATH_all_questions, PATH_all_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve questions and answers\n",
    "questions = [json_to_dataframe(path, \"questions\") for path in PATH_questions]\n",
    "answers = [json_to_dataframe(path, \"answers\") for path in PATH_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove NaNs\n",
    "questions_nan = [remove_nan_rows(question, \"question\") for question in questions]\n",
    "answers_nan = [remove_nan_rows(answer, \"answer\") for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove unnecessary columns\n",
    "questions_clean = [remove_columns(question, [\"active\", \"date_added\", \"people_id\", \"answers_ids\"]) for question in questions_nan]\n",
    "answers_clean = [remove_columns(answer, [\"active\", \"date_added\", \"people_id\", \"question_id\"]) for answer in answers_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate question & answers to generate train, test and val sets\n",
    "train = merge_dataframes_on_column(questions_clean[0], answers_clean[0], \"id\")\n",
    "test = merge_dataframes_on_column(questions_clean[1], answers_clean[1], \"id\")\n",
    "val = merge_dataframes_on_column(questions_clean[2], answers_clean[2], \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate & save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the VocabEncoder objects\n",
    "encoder_answers = VocabEncoder(PATH_all_answers, questions = False, range_numbers=False)\n",
    "vocabulary_answers = encoder_answers.getVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â Instantiate the tokenizer & BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.pooler = None  # Discard the pooling layer\n",
    "model.eval()\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625340it [00:24, 25749.53it/s]\n",
      "222684it [00:08, 26013.10it/s]\n",
      "102843it [00:03, 25927.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Token Length - Train:  35\n",
      "Max Token Length - Test:  30\n",
      "Max Token Length - Val:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Check all lengths in tokenized form\n",
    "lengths_train = length_checker(train, tokenizer=tokenizer)\n",
    "lengths_test = length_checker(test, tokenizer=tokenizer)\n",
    "lengths_val = length_checker(val, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Max Token Length - Train: \", max(lengths_train))\n",
    "print(\"Max Token Length - Test: \", max(lengths_test))\n",
    "print(\"Max Token Length - Val: \", max(lengths_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representations(dataframe, model, tokenizer, save_path, device):\n",
    "    \n",
    "    ## Batch Data Collector\n",
    "    # questions = []\n",
    "    # answers = []\n",
    "    # question_types = []\n",
    "    # img_ids = []\n",
    "    # batch_data = []\n",
    "    save_idx = 0\n",
    "\n",
    "    ## My Dataset \n",
    "    # dataset = MyDataset(dataframe)\n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    ## Iterate over batches\n",
    "    # for batch in tqdm(dataloader):\n",
    "    for idx in tqdm(range(len(dataframe))):\n",
    "            \n",
    "            \n",
    "            ## Retrieve information\n",
    "            question = dataframe.iloc[idx][\"question\"]\n",
    "            answer = dataframe.iloc[idx][\"answer\"]\n",
    "            question_type = dataframe.iloc[idx][\"type\"]\n",
    "            img_id = dataframe.iloc[idx][\"img_id\"]\n",
    "    \n",
    "            ## Retrieve batch information\n",
    "            # batch_questions = batch[0]\n",
    "            # batch_answers = batch[1]\n",
    "            # batch_question_types = batch[2]\n",
    "            # batch_img_ids = batch[3]\n",
    "    \n",
    "            ## Retrieve answer encoding\n",
    "            # batch_answers_encoded = [encoder_answers.encode(answer) for answer in batch_answers]\n",
    "            # batch_answers_tensor = [torch.tensor(answer_encoded, dtype=torch.long) for answer_encoded in batch_answers_encoded]\n",
    "            # batch_answers_tensor = torch.stack(batch_answers_tensor)\n",
    "\n",
    "            answer = encoder_answers.encode(answer)\n",
    "            answer = torch.tensor(answer, dtype=torch.long)\n",
    "    \n",
    "            ## Remove question sign from question \n",
    "            # batch_questions = [question[:-1] if question[-1:] == \"?\" else question for question in batch_questions]\n",
    "            if question[-1] == \"?\":\n",
    "                question = question[:-1]\n",
    "            else:\n",
    "                question = question\n",
    "    \n",
    "            ## Retrieve question embedding\n",
    "            # batch_tokenized = tokenizer.batch_encode_plus(batch_questions, pad_to_multiple_of=40, add_special_tokens=True, return_attention_mask=True, padding=True, return_tensors=\"pt\")\n",
    "            input = tokenizer.encode_plus(question, pad_to_multiple_of=35, add_special_tokens=True, return_attention_mask=True, padding=True, return_tensors=\"pt\")\n",
    "            input.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(**input)\n",
    "                hidden_states = output.last_hidden_state.squeeze(0).detach().cpu()\n",
    "    \n",
    "            ## Append data to batch\n",
    "            # questions.append(batch_hidden_states)\n",
    "            # answers.append(batch_answers_tensor)\n",
    "            # question_types.append(batch_question_types)\n",
    "            # img_ids.append(batch_img_ids)\n",
    "\n",
    "            ## Iterate over single entities in batches\n",
    "            # for idx in range(len(batch_questions)):\n",
    "            #     question = batch_hidden_states[idx]\n",
    "            #     answer = batch_answers_tensor[idx]\n",
    "            #     question_type = batch_question_types[idx]\n",
    "            #     img_id = batch_img_ids[idx]\n",
    "            #     img_id = img_id.item()\n",
    "    \n",
    "            ## Create data dictionary\n",
    "            data = {\n",
    "                'question': hidden_states,\n",
    "                'answer': answer,\n",
    "                'question_type': question_type,\n",
    "                'image_id': img_id\n",
    "                }\n",
    "    \n",
    "                ## Append data to batch\n",
    "                ## batch_data.append(data)\n",
    "            \n",
    "            ## Save item\n",
    "            save_path_idx = os.path.join(save_path, f\"{save_idx}.pt\")\n",
    "            torch.save(data, save_path_idx)\n",
    "            save_idx += 1\n",
    "\n",
    "            ## Update save_idx & clear all lists\n",
    "            # questions.clear()\n",
    "            # answers.clear()\n",
    "            # question_types.clear()\n",
    "            # img_ids.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 625340/625340 [2:55:18<00:00, 59.45it/s]   \n"
     ]
    }
   ],
   "source": [
    "## For Train\n",
    "create_representations(train, model, tokenizer, SAVE_PATH_TRAIN, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 222684/222684 [1:02:10<00:00, 59.69it/s]\n"
     ]
    }
   ],
   "source": [
    "## For Test\n",
    "create_representations(test, model, tokenizer, SAVE_PATH_TEST, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 102843/102843 [28:31<00:00, 60.10it/s]\n"
     ]
    }
   ],
   "source": [
    "## For Val\n",
    "create_representations(val, model, tokenizer, SAVE_PATH_VAL, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"../preprocessed_data/text_representations_bert/train/batch_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][\"question\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
